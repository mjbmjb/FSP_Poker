{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "#import cProfilev\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import Settings.arguments as arguments\n",
    "import Settings.constants as constants\n",
    "import Settings.game_settings as game_settings\n",
    "from itertools import count\n",
    "from nn.env import Env\n",
    "from nn.dqn import DQN\n",
    "from nn.dqn import DQNOptim\n",
    "from nn.net_sl import SLOptim\n",
    "from nn.table_sl import TableSL\n",
    "from nn.state import GameState\n",
    "from Tree.tree_builder import PokerTreeBuilder\n",
    "from Tree.Tests.test_tree_values import ValuesTester\n",
    "from collections import namedtuple\n",
    "builder = PokerTreeBuilder()\n",
    "\n",
    "num_episodes = 10\n",
    "env = Env()\n",
    "value_tester = ValuesTester()\n",
    "\n",
    "Agent = namedtuple('Agent',['rl','sl'])\n",
    "\n",
    "agent0 = Agent(rl=DQNOptim(),sl=SLOptim())\n",
    "agent1 = Agent(rl=DQNOptim(),sl=SLOptim())\n",
    "table_sl = agent0.sl\n",
    "agents = [agent0,agent1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def load_model(dqn_optim, iter_time):\n",
    "    iter_str = str(iter_time)\n",
    "    # load rl model (only the net)\n",
    "    dqn_optim.model.load_state_dict(torch.load('../Data/Model/Iter:' + iter_str + '.rl'))\n",
    "    # load sl model\n",
    "    table_sl.strategy = torch.load('../Data/Model/Iter:' + iter_str + '.sl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(episod):\n",
    "    path = '../Data/Model/'\n",
    "    sl_name = path + \"Iter:\" + str(episod) + '.sl'\n",
    "    rl_name = path + \"Iter:\" + str(episod)\n",
    "    memory_name = path + 'Iter:' + str(episod)   \n",
    "    # save sl strategy\n",
    "#    torch.save(table_sl.strategy, sl_name)\n",
    "    # save rl strategy\n",
    "    # 1.0 save the prarmeter\n",
    "    torch.save(agent0.rl.model.state_dict(), rl_name + '_0_' + '.rl')\n",
    "    torch.save(agent1.rl.model.state_dict(), rl_name + '_1_' + '.rl')\n",
    "    # 2.0 save the memory of DQN\n",
    "    np.save(memory_name, np.array(agent0.rl.memory.memory))\n",
    "    np.save(memory_name, np.array(agent1.rl.memory.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = 0\n",
    "#@profile\n",
    "def main():\n",
    "    print('I have start')\n",
    "    import time\n",
    "    time_start = time.time()\n",
    "    total_reward = 0.0\n",
    "    table_update_num = 0\n",
    "    \n",
    "    if arguments.load_model:\n",
    "        load_model(dqn_optim, arguments.load_model_num)\n",
    "    \n",
    "    for i_episode in range(arguments.epoch_count + 1):\n",
    "        agents[0], agents[1] = agents[1], agents[0] \n",
    "        \n",
    "        # choose policy 0-sl 1-rl\n",
    "        flag = 0 if random.random() > arguments.eta else 1\n",
    "        \n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        for t in count():\n",
    "            state_tensor = builder.statenode_to_tensor(state)\n",
    "            current_player = state.node.current_player\n",
    "            # Select and perform an action\n",
    "#            print(state_tensor.size(1))\n",
    "            assert(state_tensor.size(1) == 23)\n",
    "            \n",
    "            if flag == 0:\n",
    "                # sl\n",
    "                action = agents[current_player].sl.select_action(state_tensor)\n",
    "            elif flag == 1:\n",
    "                #rl\n",
    "                action = agents[current_player].rl.select_action(state_tensor)\n",
    "            else:\n",
    "                assert(False)\n",
    "                \n",
    "            next_state, real_next_state, reward, done = env.step(agents[1-current_player], state, action)\n",
    "#            reward = reward / 2400.0\n",
    "            \n",
    "            # transform to tensor\n",
    "            real_next_state_tensor = builder.statenode_to_tensor(real_next_state)\n",
    "            \n",
    "            action_tensor = action\n",
    "\n",
    "            # Store the transition in reforcement learning memory Mrl\n",
    "            agents[current_player].rl.memory.push(state_tensor, action_tensor, real_next_state_tensor, arguments.Tensor([reward]))\n",
    "                \n",
    "            training_flag = False\n",
    "            if True or len(agents[current_player].rl.memory.memory) == agents[current_player].rl.memory.capacity:\n",
    "                training_flag = True\n",
    "                if flag == 1:\n",
    "                    # if choose sl store tuple(s,a) in supervised learning memory Msl\n",
    "                    agents[current_player].sl.memory.push(state_tensor, action_tensor[0])\n",
    "                    table_update_num = table_update_num + 1\n",
    "                    if table_update_num >= arguments.sl_update_num:\n",
    "#                        agents[current_player].sl.update_strategy()\n",
    "                        agents[current_player].sl.optimize_model()\n",
    "#                        agents[current_player].sl.plot_error_vis(i_episode)\n",
    "                        table_update_num = 0\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                agents[current_player].rl.optimize_model() \n",
    "                # Move to the next state\n",
    "            state = next_state\n",
    "    \n",
    "            # update the target net work\n",
    "            if agents[current_player].rl.steps_done > 0 and agents[current_player].rl.steps_done % 300 == 0 and training_flag:\n",
    "                agents[current_player].rl.target_net.load_state_dict(agents[current_player].rl.model.state_dict())\n",
    "                agents[current_player].rl.plot_error_vis(i_episode)\n",
    "                agents[current_player].sl.plot_error_vis(i_episode)\n",
    "            \n",
    "#            if i_episode % 1000 == 0 and training_flag:\n",
    "#                print(len(agents[current_player].rl.memory.memory))\n",
    "#               agents[current_player].rl.plot_error_vis(i_episode)\n",
    "            \n",
    "            if done:\n",
    "#                if(i_episode % 100 == 0 and training_flag):\n",
    "#                    agents[current_player].rl.plot_error_vis(i_episode)\n",
    "                if(i_episode % arguments.save_epoch == 0 and training_flag):\n",
    "                    save_model(i_episode)\n",
    "#                    value_tester.test(agents[current_player].sl.strategy.clone(), i_episode)\n",
    "#                    save_table_csv(table_sl.strategy)\n",
    "#                dqn_optim.episode_durations.append(t + 1)\n",
    "#                dqn_optim.plot_durations()\n",
    "                break\n",
    "    print(len(agents[current_player].rl.memory.memory))    \n",
    "#    dqn_optim.plot_error()\n",
    "#    global LOSS_ACC\n",
    "#    LOSS_ACC = dqn_optim.error_acc\n",
    "    # save the model\n",
    "    if arguments.load_model:\n",
    "        i_episode = i_episode + arguments.load_model_num\n",
    "            \n",
    "            \n",
    "    print('Complete')\n",
    "    print((time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
